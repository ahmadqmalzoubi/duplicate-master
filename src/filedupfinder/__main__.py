from .cli import parse_args
from .logger import setup_logger
from .deduper import find_duplicates
from .analyzer import analyze_space_savings, format_bytes
from .deletion import handle_deletion
from .exporter import export_results
from .demo import run_demo
from .benchmark import run_benchmark
import os


def main() -> None:
    """
    Main entry point for the duplicate file finder application.

    This function orchestrates the entire duplicate file finding process:
    1. Parses command-line arguments
    2. Sets up logging
    3. Validates the target directory
    4. Performs the duplicate file scan
    5. Analyzes and reports results
    6. Handles deletion if requested
    7. Exports results if specified

    The function provides comprehensive feedback to the user including:
    - Scan progress and status
    - Summary statistics (groups, files, space usage)
    - Error handling and logging
    - Export functionality

    Examples:
        >>> main()
        # Scans current directory and reports results

    Command-line usage:
        $ filedupfinder /path/to/scan
        $ filedupfinder --delete --dry-run /path/to/scan
        $ filedupfinder --minsize 5 --maxsize 500 /path/to/scan
        $ filedupfinder --demo
        $ filedupfinder --benchmark

    Note:
        - Exits with error code 1 if the target directory is invalid
        - Provides detailed logging for debugging
        - Supports both scan-only and deletion modes
        - Handles export to JSON/CSV formats
        - Supports demo mode for testing and demonstration
        - Supports performance benchmarking
        - Supports legacy scanning mode for comparison
    """
    args = parse_args()
    logger = setup_logger(args)

    # Check for demo mode
    if args.demo:
        run_demo()
        return

    # Check for benchmark mode
    if args.benchmark:
        run_benchmark()
        return

    if not os.path.exists(args.basedir) or not os.path.isdir(args.basedir):
        logger.error(f"Invalid directory: {args.basedir}")
        return

    duplicates = find_duplicates(
        base_dir=os.path.abspath(args.basedir),
        min_size=args.minsize,
        max_size=args.maxsize,
        quick_mode=args.quick,
        multi_region=args.multi_region,
        exclude=args.exclude,
        exclude_dir=args.exclude_dir,
        exclude_hidden=args.exclude_hidden,
        threads=args.threads,
        logger=logger,
        use_optimized_scanning=not args.legacy_scan
    )

    total_space, savings = analyze_space_savings(duplicates)
    num_groups = len(duplicates)
    num_files = sum(len(paths) for paths in duplicates.values())

    logger.info("\nðŸ“Š Scan Summary:")
    logger.info(f"   â€¢ {num_groups} duplicate groups detected")
    logger.info(f"   â€¢ {num_files} duplicate files in total")
    logger.info(
        f"   â€¢ {format_bytes(total_space)} of space used by duplicates")
    logger.info(f"   â€¢ {format_bytes(savings)} can be reclaimed")

    if num_groups == 0:
        logger.info("   â€¢ No duplicate files found in the scanned directory.")

    if args.delete:
        handle_deletion(duplicates, args, logger)

    export_results(duplicates, args, logger)


if __name__ == "__main__":
    main()
